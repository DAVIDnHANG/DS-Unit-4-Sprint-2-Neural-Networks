{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pBQsZEJmubLs"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "\n",
    "# Neural Network Framework (Keras)\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignmnet 3*\n",
    "\n",
    "## Use the Keras Library to build a Multi-Layer Perceptron Model on the Boston Housing dataset\n",
    "\n",
    "- The Boston Housing dataset comes with the Keras library so use Keras to import it into your notebook. \n",
    "- Normalize the data (all features should have roughly the same scale)\n",
    "- Import the type of model and layers that you will need from Keras.\n",
    "- Instantiate a model object and use `model.add()` to add layers to your model\n",
    "- Since this is a regression model you will have a single output node in the final layer.\n",
    "- Use activation functions that are appropriate for this task\n",
    "- Compile your model\n",
    "- Fit your model and report its accuracy in terms of Mean Squared Error\n",
    "- Use the history object that is returned from model.fit to make graphs of the model's loss or train/validation accuracies by epoch. \n",
    "- Run this same data through a linear regression model. Which achieves higher accuracy?\n",
    "- Do a little bit of feature engineering and see how that affects your neural network model. (you will need to change your model to accept more inputs)\n",
    "- After feature engineering, which model sees a greater accuracy boost due to the new features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8NLTAR87uYJ-"
   },
   "outputs": [],
   "source": [
    "#load_boston\n",
    "#- The Boston Housing dataset comes with the Keras library so use Keras to import it into your notebook. \n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "Boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n",
       "        4.9800e+00],\n",
       "       [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n",
       "        9.1400e+00],\n",
       "       [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n",
       "        4.0300e+00],\n",
       "       ...,\n",
       "       [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "        5.6400e+00],\n",
       "       [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n",
       "        6.4800e+00],\n",
       "       [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "        7.8800e+00]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = Boston.data\n",
    "Y = Boston.target \n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.26388341e-05, 3.59966795e-02, 4.61957387e-03, ...,\n",
       "        3.05971776e-02, 7.93726783e-01, 9.95908132e-03],\n",
       "       [5.78529889e-05, 0.00000000e+00, 1.49769546e-02, ...,\n",
       "        3.77071843e-02, 8.40785474e-01, 1.93620036e-02],\n",
       "       [5.85729947e-05, 0.00000000e+00, 1.51744622e-02, ...,\n",
       "        3.82044450e-02, 8.43137761e-01, 8.64965806e-03],\n",
       "       ...,\n",
       "       [1.23765824e-04, 0.00000000e+00, 2.43009593e-02, ...,\n",
       "        4.27762066e-02, 8.08470305e-01, 1.14884669e-02],\n",
       "       [2.24644719e-04, 0.00000000e+00, 2.44548909e-02, ...,\n",
       "        4.30471676e-02, 8.06519433e-01, 1.32831260e-02],\n",
       "       [9.69214289e-05, 0.00000000e+00, 2.43887924e-02, ...,\n",
       "        4.29308164e-02, 8.11392431e-01, 1.61092778e-02]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalized mean used preprocessing class from sklearn then use the normalize function.\n",
    "from sklearn import preprocessing\n",
    "#normalized refer to rescaling real valued numeric attributes into 0 and 1.\n",
    "#\n",
    "normalized_x = preprocessing.normalize(X)\n",
    "normalized_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import the type of model and layers that you will need from Keras.\n",
    "#- Instantiate a model object and use `model.add()` to add layers to your model\n",
    "#- Use activation functions that are appropriate for this task\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim=13, \\\n",
    "                    kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import numpy\n",
    "\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "#- Since this is a regression model you will have a single output node in the final layer.\n",
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=10,batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(estimator, X, Y, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "506/506 [==============================] - 1s 2ms/step - loss: 266.7740\n",
      "Epoch 2/150\n",
      "506/506 [==============================] - 0s 206us/step - loss: 108.6745\n",
      "Epoch 3/150\n",
      "506/506 [==============================] - 0s 209us/step - loss: 78.9845\n",
      "Epoch 4/150\n",
      "506/506 [==============================] - 0s 208us/step - loss: 70.8852\n",
      "Epoch 5/150\n",
      "506/506 [==============================] - 0s 212us/step - loss: 65.8467\n",
      "Epoch 6/150\n",
      "506/506 [==============================] - 0s 268us/step - loss: 63.6887\n",
      "Epoch 7/150\n",
      "506/506 [==============================] - 0s 222us/step - loss: 61.3821\n",
      "Epoch 8/150\n",
      "506/506 [==============================] - 0s 207us/step - loss: 60.3063\n",
      "Epoch 9/150\n",
      "506/506 [==============================] - 0s 207us/step - loss: 59.2684\n",
      "Epoch 10/150\n",
      "506/506 [==============================] - 0s 218us/step - loss: 56.9353\n",
      "Epoch 11/150\n",
      "506/506 [==============================] - 0s 218us/step - loss: 55.7252\n",
      "Epoch 12/150\n",
      "506/506 [==============================] - 0s 207us/step - loss: 53.8524\n",
      "Epoch 13/150\n",
      "506/506 [==============================] - 0s 209us/step - loss: 52.9021\n",
      "Epoch 14/150\n",
      "506/506 [==============================] - 0s 209us/step - loss: 53.7465\n",
      "Epoch 15/150\n",
      "506/506 [==============================] - 0s 210us/step - loss: 50.2799\n",
      "Epoch 16/150\n",
      "506/506 [==============================] - 0s 191us/step - loss: 50.0473\n",
      "Epoch 17/150\n",
      "506/506 [==============================] - 0s 206us/step - loss: 48.1709\n",
      "Epoch 18/150\n",
      "506/506 [==============================] - 0s 205us/step - loss: 47.7962\n",
      "Epoch 19/150\n",
      "506/506 [==============================] - 0s 216us/step - loss: 45.3237\n",
      "Epoch 20/150\n",
      "506/506 [==============================] - 0s 208us/step - loss: 44.5362\n",
      "Epoch 21/150\n",
      "506/506 [==============================] - 0s 210us/step - loss: 43.0611\n",
      "Epoch 22/150\n",
      "506/506 [==============================] - 0s 207us/step - loss: 43.3914\n",
      "Epoch 23/150\n",
      "506/506 [==============================] - 0s 207us/step - loss: 40.7605\n",
      "Epoch 24/150\n",
      "506/506 [==============================] - 0s 218us/step - loss: 39.8249\n",
      "Epoch 25/150\n",
      "506/506 [==============================] - 0s 215us/step - loss: 39.6234\n",
      "Epoch 26/150\n",
      "506/506 [==============================] - 0s 215us/step - loss: 38.6793\n",
      "Epoch 27/150\n",
      "506/506 [==============================] - 0s 209us/step - loss: 38.3232\n",
      "Epoch 28/150\n",
      "506/506 [==============================] - 0s 209us/step - loss: 36.5540\n",
      "Epoch 29/150\n",
      "506/506 [==============================] - 0s 210us/step - loss: 36.1751\n",
      "Epoch 30/150\n",
      "506/506 [==============================] - 0s 207us/step - loss: 35.4617\n",
      "Epoch 31/150\n",
      "506/506 [==============================] - 0s 218us/step - loss: 35.6172\n",
      "Epoch 32/150\n",
      "506/506 [==============================] - 0s 209us/step - loss: 35.1086\n",
      "Epoch 33/150\n",
      "506/506 [==============================] - 0s 212us/step - loss: 33.8019\n",
      "Epoch 34/150\n",
      "506/506 [==============================] - 0s 212us/step - loss: 34.7500\n",
      "Epoch 35/150\n",
      "506/506 [==============================] - 0s 211us/step - loss: 32.7307\n",
      "Epoch 36/150\n",
      "506/506 [==============================] - 0s 207us/step - loss: 32.9023\n",
      "Epoch 37/150\n",
      "506/506 [==============================] - 0s 210us/step - loss: 32.8326\n",
      "Epoch 38/150\n",
      "506/506 [==============================] - 0s 210us/step - loss: 31.5874\n",
      "Epoch 39/150\n",
      "506/506 [==============================] - 0s 207us/step - loss: 32.3244\n",
      "Epoch 40/150\n",
      "506/506 [==============================] - 0s 211us/step - loss: 32.5783\n",
      "Epoch 41/150\n",
      "506/506 [==============================] - 0s 210us/step - loss: 30.9232\n",
      "Epoch 42/150\n",
      "506/506 [==============================] - 0s 215us/step - loss: 31.1146\n",
      "Epoch 43/150\n",
      "506/506 [==============================] - 0s 211us/step - loss: 31.8288\n",
      "Epoch 44/150\n",
      "506/506 [==============================] - 0s 210us/step - loss: 31.6404\n",
      "Epoch 45/150\n",
      "506/506 [==============================] - 0s 210us/step - loss: 30.4064\n",
      "Epoch 46/150\n",
      "506/506 [==============================] - 0s 217us/step - loss: 30.7357\n",
      "Epoch 47/150\n",
      "506/506 [==============================] - 0s 207us/step - loss: 30.1611\n",
      "Epoch 48/150\n",
      "506/506 [==============================] - 0s 211us/step - loss: 29.9459\n",
      "Epoch 49/150\n",
      "506/506 [==============================] - 0s 218us/step - loss: 29.3110\n",
      "Epoch 50/150\n",
      "506/506 [==============================] - 0s 209us/step - loss: 30.7256\n",
      "Epoch 51/150\n",
      "506/506 [==============================] - 0s 209us/step - loss: 29.0515\n",
      "Epoch 52/150\n",
      "506/506 [==============================] - 0s 209us/step - loss: 30.5041\n",
      "Epoch 53/150\n",
      "506/506 [==============================] - 0s 209us/step - loss: 29.7786\n",
      "Epoch 54/150\n",
      "506/506 [==============================] - 0s 217us/step - loss: 28.7326\n",
      "Epoch 55/150\n",
      "506/506 [==============================] - 0s 217us/step - loss: 28.9632\n",
      "Epoch 56/150\n",
      "506/506 [==============================] - 0s 211us/step - loss: 28.2908\n",
      "Epoch 57/150\n",
      "506/506 [==============================] - 0s 210us/step - loss: 29.3694\n",
      "Epoch 58/150\n",
      "506/506 [==============================] - 0s 212us/step - loss: 27.7936\n",
      "Epoch 59/150\n",
      "506/506 [==============================] - 0s 210us/step - loss: 27.6705\n",
      "Epoch 60/150\n",
      "506/506 [==============================] - 0s 215us/step - loss: 27.6454\n",
      "Epoch 61/150\n",
      "506/506 [==============================] - 0s 221us/step - loss: 27.4191\n",
      "Epoch 62/150\n",
      "506/506 [==============================] - 0s 216us/step - loss: 27.6199\n",
      "Epoch 63/150\n",
      "506/506 [==============================] - 0s 210us/step - loss: 27.3373\n",
      "Epoch 64/150\n",
      "506/506 [==============================] - 0s 211us/step - loss: 27.1303\n",
      "Epoch 65/150\n",
      "506/506 [==============================] - 0s 210us/step - loss: 27.1915\n",
      "Epoch 66/150\n",
      "506/506 [==============================] - 0s 211us/step - loss: 26.7058\n",
      "Epoch 67/150\n",
      "506/506 [==============================] - 0s 210us/step - loss: 27.0078\n",
      "Epoch 68/150\n",
      "506/506 [==============================] - 0s 209us/step - loss: 26.2440\n",
      "Epoch 69/150\n",
      "506/506 [==============================] - 0s 210us/step - loss: 27.0226\n",
      "Epoch 70/150\n",
      "506/506 [==============================] - 0s 217us/step - loss: 25.9117\n",
      "Epoch 71/150\n",
      "506/506 [==============================] - 0s 209us/step - loss: 26.1649\n",
      "Epoch 72/150\n",
      "506/506 [==============================] - 0s 201us/step - loss: 25.5852\n",
      "Epoch 73/150\n",
      "506/506 [==============================] - 0s 210us/step - loss: 26.4422\n",
      "Epoch 74/150\n",
      "506/506 [==============================] - 0s 209us/step - loss: 25.0729\n",
      "Epoch 75/150\n",
      "506/506 [==============================] - 0s 215us/step - loss: 24.7548\n",
      "Epoch 76/150\n",
      "506/506 [==============================] - 0s 209us/step - loss: 24.4513\n",
      "Epoch 77/150\n",
      "506/506 [==============================] - 0s 216us/step - loss: 23.8503\n",
      "Epoch 78/150\n",
      "506/506 [==============================] - 0s 216us/step - loss: 24.1764\n",
      "Epoch 79/150\n",
      "506/506 [==============================] - 0s 217us/step - loss: 23.7673\n",
      "Epoch 80/150\n",
      "506/506 [==============================] - 0s 211us/step - loss: 24.6660\n",
      "Epoch 81/150\n",
      "506/506 [==============================] - 0s 195us/step - loss: 23.7556\n",
      "Epoch 82/150\n",
      "506/506 [==============================] - 0s 216us/step - loss: 23.5236\n",
      "Epoch 83/150\n",
      "506/506 [==============================] - 0s 216us/step - loss: 23.9495\n",
      "Epoch 84/150\n",
      "506/506 [==============================] - 0s 207us/step - loss: 24.7488\n",
      "Epoch 85/150\n",
      "506/506 [==============================] - 0s 209us/step - loss: 22.9960\n",
      "Epoch 86/150\n",
      "506/506 [==============================] - 0s 212us/step - loss: 24.0175\n",
      "Epoch 87/150\n",
      "506/506 [==============================] - 0s 210us/step - loss: 24.4194\n",
      "Epoch 88/150\n",
      "506/506 [==============================] - 0s 215us/step - loss: 22.4544\n",
      "Epoch 89/150\n",
      "506/506 [==============================] - 0s 210us/step - loss: 22.3610\n",
      "Epoch 90/150\n",
      "506/506 [==============================] - 0s 219us/step - loss: 22.5545\n",
      "Epoch 91/150\n",
      "506/506 [==============================] - 0s 209us/step - loss: 23.4596\n",
      "Epoch 92/150\n",
      "506/506 [==============================] - 0s 217us/step - loss: 22.8146\n",
      "Epoch 93/150\n",
      "506/506 [==============================] - 0s 219us/step - loss: 23.2734\n",
      "Epoch 94/150\n",
      "506/506 [==============================] - 0s 221us/step - loss: 21.6951\n",
      "Epoch 95/150\n",
      "506/506 [==============================] - 0s 208us/step - loss: 23.1967\n",
      "Epoch 96/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 210us/step - loss: 21.8041\n",
      "Epoch 97/150\n",
      "506/506 [==============================] - 0s 216us/step - loss: 21.8827\n",
      "Epoch 98/150\n",
      "506/506 [==============================] - 0s 218us/step - loss: 21.5620\n",
      "Epoch 99/150\n",
      "506/506 [==============================] - 0s 218us/step - loss: 22.2887\n",
      "Epoch 100/150\n",
      "506/506 [==============================] - 0s 211us/step - loss: 21.9266\n",
      "Epoch 101/150\n",
      "506/506 [==============================] - 0s 211us/step - loss: 21.4985\n",
      "Epoch 102/150\n",
      "506/506 [==============================] - 0s 219us/step - loss: 21.2514\n",
      "Epoch 103/150\n",
      "506/506 [==============================] - 0s 210us/step - loss: 20.8286\n",
      "Epoch 104/150\n",
      "506/506 [==============================] - 0s 209us/step - loss: 21.1448\n",
      "Epoch 105/150\n",
      "506/506 [==============================] - 0s 212us/step - loss: 20.3371\n",
      "Epoch 106/150\n",
      "506/506 [==============================] - 0s 217us/step - loss: 20.7750\n",
      "Epoch 107/150\n",
      "506/506 [==============================] - 0s 211us/step - loss: 20.5609\n",
      "Epoch 108/150\n",
      "506/506 [==============================] - 0s 210us/step - loss: 20.4983\n",
      "Epoch 109/150\n",
      "506/506 [==============================] - 0s 197us/step - loss: 20.5654\n",
      "Epoch 110/150\n",
      "506/506 [==============================] - 0s 216us/step - loss: 21.9126\n",
      "Epoch 111/150\n",
      "506/506 [==============================] - 0s 210us/step - loss: 20.5551\n",
      "Epoch 112/150\n",
      "506/506 [==============================] - 0s 211us/step - loss: 20.2170\n",
      "Epoch 113/150\n",
      "506/506 [==============================] - 0s 219us/step - loss: 19.7150\n",
      "Epoch 114/150\n",
      "506/506 [==============================] - 0s 210us/step - loss: 20.2515\n",
      "Epoch 115/150\n",
      "506/506 [==============================] - 0s 207us/step - loss: 19.8581\n",
      "Epoch 116/150\n",
      "506/506 [==============================] - 0s 217us/step - loss: 19.8588\n",
      "Epoch 117/150\n",
      "506/506 [==============================] - 0s 210us/step - loss: 20.7459\n",
      "Epoch 118/150\n",
      "506/506 [==============================] - 0s 216us/step - loss: 19.1728\n",
      "Epoch 119/150\n",
      "506/506 [==============================] - 0s 210us/step - loss: 19.4688\n",
      "Epoch 120/150\n",
      "506/506 [==============================] - 0s 210us/step - loss: 19.7366\n",
      "Epoch 121/150\n",
      "506/506 [==============================] - 0s 210us/step - loss: 20.0846\n",
      "Epoch 122/150\n",
      "506/506 [==============================] - 0s 213us/step - loss: 19.1209\n",
      "Epoch 123/150\n",
      "506/506 [==============================] - 0s 213us/step - loss: 20.7538\n",
      "Epoch 124/150\n",
      "506/506 [==============================] - 0s 212us/step - loss: 21.7944\n",
      "Epoch 125/150\n",
      "506/506 [==============================] - 0s 210us/step - loss: 20.2105\n",
      "Epoch 126/150\n",
      "506/506 [==============================] - 0s 217us/step - loss: 19.5851\n",
      "Epoch 127/150\n",
      "506/506 [==============================] - 0s 215us/step - loss: 20.3824\n",
      "Epoch 128/150\n",
      "506/506 [==============================] - 0s 209us/step - loss: 19.5914\n",
      "Epoch 129/150\n",
      "506/506 [==============================] - 0s 210us/step - loss: 19.5558\n",
      "Epoch 130/150\n",
      "506/506 [==============================] - 0s 210us/step - loss: 18.6149\n",
      "Epoch 131/150\n",
      "506/506 [==============================] - 0s 209us/step - loss: 19.0400\n",
      "Epoch 132/150\n",
      "506/506 [==============================] - 0s 209us/step - loss: 19.6512\n",
      "Epoch 133/150\n",
      "506/506 [==============================] - 0s 208us/step - loss: 19.4107\n",
      "Epoch 134/150\n",
      "506/506 [==============================] - 0s 215us/step - loss: 19.7751\n",
      "Epoch 135/150\n",
      "506/506 [==============================] - 0s 222us/step - loss: 19.0647\n",
      "Epoch 136/150\n",
      "506/506 [==============================] - 0s 209us/step - loss: 18.9578\n",
      "Epoch 137/150\n",
      "506/506 [==============================] - 0s 211us/step - loss: 19.3646\n",
      "Epoch 138/150\n",
      "506/506 [==============================] - 0s 208us/step - loss: 18.9170\n",
      "Epoch 139/150\n",
      "506/506 [==============================] - 0s 216us/step - loss: 19.2658\n",
      "Epoch 140/150\n",
      "506/506 [==============================] - 0s 208us/step - loss: 19.7293\n",
      "Epoch 141/150\n",
      "506/506 [==============================] - 0s 219us/step - loss: 19.3152\n",
      "Epoch 142/150\n",
      "506/506 [==============================] - 0s 208us/step - loss: 19.5183\n",
      "Epoch 143/150\n",
      "506/506 [==============================] - 0s 212us/step - loss: 18.6516\n",
      "Epoch 144/150\n",
      "506/506 [==============================] - 0s 236us/step - loss: 19.1026\n",
      "Epoch 145/150\n",
      "506/506 [==============================] - 0s 217us/step - loss: 18.9944\n",
      "Epoch 146/150\n",
      "506/506 [==============================] - 0s 199us/step - loss: 18.7320\n",
      "Epoch 147/150\n",
      "506/506 [==============================] - 0s 218us/step - loss: 18.0583\n",
      "Epoch 148/150\n",
      "506/506 [==============================] - 0s 210us/step - loss: 19.0407\n",
      "Epoch 149/150\n",
      "506/506 [==============================] - 0s 210us/step - loss: 18.3925\n",
      "Epoch 150/150\n",
      "506/506 [==============================] - 0s 212us/step - loss: 19.0763\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal')) \n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "history=model.fit(X, Y, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Compile your model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse', 'mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Fit your model and report its accuracy in terms of Mean Squared Error\n",
    "history = model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_mean_squared_error', 'val_mean_absolute_error', 'loss', 'mean_squared_error', 'mean_absolute_error'])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-878bda91b59b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# summarize history for accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'accuracy'"
     ]
    }
   ],
   "source": [
    "\n",
    "#- Use the history object that is returned from model.fit to make graphs of the model's loss or train/validation accuracies by epoch. \n",
    "#ne of the default callbacks that is registered when training all deep learning models is the History callback.\n",
    "#It records training metrics for each epoch. \n",
    "#This includes the loss and the accuracy (for classification problems) as well as the loss and accuracy\n",
    "#for the validation dataset, if one is set.\n",
    "# Visualize training history\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Run this same data through a linear regression model. Which achieves higher accuracy?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Do a little bit of feature engineering and see how that affects your neural network model. (you will need to change your model to accept more inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- After feature engineering, which model sees a greater accuracy boost due to the new features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SfcFnOONyuNm"
   },
   "source": [
    "## Use the Keras Library to build an image recognition network using the Fashion-MNIST dataset (also comes with keras)\n",
    "\n",
    "- Load and preprocess the image data similar to how we preprocessed the MNIST data in class.\n",
    "- Make sure to one-hot encode your category labels\n",
    "- Make sure to have your final layer have as many nodes as the number of classes that you want to predict.\n",
    "- Try different hyperparameters. What is the highest accuracy that you are able to achieve.\n",
    "- Use the history object that is returned from model.fit to make graphs of the model's loss or train/validation accuracies by epoch. \n",
    "- Remember that neural networks fall prey to randomness so you may need to run your model multiple times (or use Cross Validation) in order to tell if a change to a hyperparameter is truly producing better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "szi6-IpuzaH1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "##### Your Code Here #####\n",
    "# import library and import images\n",
    "import tensorflow as tf\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "Number of images in x_train 60000\n",
      "Number of images in x_test 10000\n"
     ]
    }
   ],
   "source": [
    "# Reshaping the array to 4-dims so that it can work with the Keras API\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "input_shape = (28, 28, 1)\n",
    "# Making sure that the values are float so that we can get decimal points after division\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "# Normalizing the RGB codes by dividing it to the max RGB value.\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('Number of images in x_train', x_train.shape[0])\n",
    "print('Number of images in x_test', x_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Importing the required Keras modules containing model and layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "# Creating a Sequential Model and adding the layers\n",
    "model = Sequential()\n",
    "model.add(Conv2D(28, kernel_size=(3,3), input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
    "model.add(Dense(128, activation=tf.nn.relu))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10,activation=tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 37s 616us/step - loss: 0.6393 - acc: 0.8063\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 36s 595us/step - loss: 0.2474 - acc: 0.9257\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 36s 599us/step - loss: 0.1846 - acc: 0.9443\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 36s 594us/step - loss: 0.1536 - acc: 0.9548\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 37s 624us/step - loss: 0.1348 - acc: 0.9593\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 35s 588us/step - loss: 0.1193 - acc: 0.9632\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 35s 590us/step - loss: 0.1068 - acc: 0.9671\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 35s 584us/step - loss: 0.0991 - acc: 0.9701\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 35s 588us/step - loss: 0.0918 - acc: 0.9716\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 37s 621us/step - loss: 0.0863 - acc: 0.9731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f15b59d6cf8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "model.fit(x=x_train,y=y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 233us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07340203110827133, 0.9775]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img_rows' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-8be3f3309b74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimage_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4444\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Greys'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'img_rows' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_index = 4444\n",
    "plt.imshow(x_test[image_index].reshape(28, 28),cmap='Greys')\n",
    "pred = model.predict(x_test[image_index].reshape(1, img_rows, img_cols, 1))\n",
    "print(pred.argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zv_3xNMjzdLI"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Use Hyperparameter Tuning to make the accuracy of your models as high as possible. (error as low as possible)\n",
    "- Use Cross Validation techniques to get more consistent results with your model.\n",
    "- Use GridSearchCV to try different combinations of hyperparameters. \n",
    "- Start looking into other types of Keras layers for CNNs and RNNs maybe try and build a CNN model for fashion-MNIST to see how the results compare."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS_DS_433_Keras_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
